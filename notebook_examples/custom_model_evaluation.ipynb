{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531a72bd-4366-4904-aecf-b2f5f0eb9dc3",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "This notebook has example code to be able to train a custom model and visualise performance using Vertex AI Pipelines. This is a regression model predicting the total fare of taxi trips from the BigQuery Chicago Taxi Trips dataset. The datasets were created using the query\n",
    "\n",
    "SELECT\n",
    "  CAST(EXTRACT(DAYOFWEEK\n",
    "    FROM\n",
    "      trip_start_timestamp) AS FLOAT64) AS dayofweek,\n",
    "  CAST(EXTRACT(HOUR\n",
    "    FROM\n",
    "      trip_start_timestamp) AS FLOAT64) AS hourofday,\n",
    "  ST_DISTANCE( ST_GEOGPOINT(pickup_longitude, pickup_latitude), ST_GEOGPOINT(dropoff_longitude, dropoff_latitude)) AS trip_distance,\n",
    "  trip_miles,\n",
    "  CAST(\n",
    "    CASE\n",
    "      WHEN trip_seconds IS NULL THEN 1\n",
    "      WHEN trip_seconds <= 0 THEN 1\n",
    "    ELSE\n",
    "    trip_seconds\n",
    "  END\n",
    "    AS FLOAT64) AS trip_seconds,\n",
    "  (fare + tips + tolls + extras) AS total_fare,\n",
    "FROM\n",
    "  `<CHICAGO_TAXI_DATA>`\n",
    "WHERE\n",
    "  trip_miles > 0\n",
    "  AND fare > 0\n",
    "  AND fare < 1500\n",
    "  AND fare IS NOT NULL\n",
    "  AND tips IS NOT NULL\n",
    "  AND tolls IS NOT NULL\n",
    "  AND extras IS NOT NULL \n",
    "  AND trip_start_timestamp IS NOT NULL\n",
    "  AND pickup_longitude IS NOT NULL\n",
    "  AND pickup_latitude IS NOT NULL\n",
    "  AND dropoff_longitude IS NOT NULL\n",
    "  AND dropoff_latitude IS NOT NULL\n",
    "  AND payment_type IS NOT NULL\n",
    "  AND company IS NOT NULL\n",
    "  AND trip_start_timestamp BETWEEN \"2021-01-01\"\n",
    "  AND \"2021-12-31\"\n",
    "LIMIT\n",
    "  10000\n",
    "  \n",
    "The pipeline assumes that the training, validation, and test datasets have already been created in BigQuery and exported to GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af9309-cd86-4a9f-89f8-aecffc670a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries to submit pipeline\n",
    "# !pip install kfp==1.8.17 google-cloud-pipeline-components==1.0.37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9125b-4d3f-455e-a0e3-6849d8c463e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact, Input, Output, Dataset, Model, component\n",
    "from kfp.v2.components import importer_node\n",
    "\n",
    "from google_cloud_pipeline_components.experimental.custom_job.utils import (\n",
    "    create_custom_training_job_op_from_component\n",
    ")\n",
    "\n",
    "from google_cloud_pipeline_components.aiplatform import ModelBatchPredictOp, ModelUploadOp\n",
    "from google_cloud_pipeline_components.experimental.evaluation import (\n",
    "        ModelEvaluationFeatureAttributionOp, ModelEvaluationRegressionOp,\n",
    "        ModelImportEvaluationOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9a559-b47b-47ee-9049-47c246502e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"<PROJECT_ID>\"\n",
    "dataset_id = \"<DATASET_ID>\"\n",
    "bq_training_data = f\"bq://{project_id}.{dataset_id}.training_data\"\n",
    "bq_validation_data = f\"bq://{project_id}.{dataset_id}.validation_data\"\n",
    "bq_test_data = f\"bq://{project_id}.{dataset_id}.test_data\"\n",
    "bq_test_data_no_label = f\"bq://{project_id}.{dataset_id}.test_data_no_label\"\n",
    "gcs_training_data = \"gs://<GCS_BUCKET>/training_data.csv\"\n",
    "gcs_validation_data = \"gs://<GCS_BUCKET>/validation_data.csv\"\n",
    "gcs_test_data = \"gs://<GCS_BUCKET>/test_data.csv\"\n",
    "label_name = \"total_fare\"\n",
    "pipeline_root = \"gs://<GCS_ROOT_BUCKET>\"\n",
    "location = \"europe-west1\"\n",
    "service_account = \"<SERVICE_ACCOUNT>\"\n",
    "subnetwork = \"<SUBNETWORK>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3d1e6-fe79-46e1-88b8-82d11356f52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    base_image=\"python:3.7\",\n",
    "    packages_to_install=[\"xgboost==1.4.2\", \"scikit-learn==0.24.2\", \"pandas==1.3.2\", \"gcsfs==0.7.1\", \"pyarrow==8.0.0\"],\n",
    ")\n",
    "def train_xgboost_model(\n",
    "    training_data: str,\n",
    "    validation_data: str,\n",
    "    label_name: str,\n",
    "    model_params: dict,\n",
    "    model: Output[Model],\n",
    "    metrics_artifact: Output[Artifact],\n",
    ") -> None:\n",
    "    \"\"\"Train an XGBoost model.\n",
    "    Args:\n",
    "        training_data (str): GCS location of training data.\n",
    "        validation_data (str): GCS location of validation data.\n",
    "        label_name (str): CSV column name containing the label data.\n",
    "        model_params (dict): Dictionary of following training parameters\n",
    "            num_iterations: int - Number of boosting iterations.\n",
    "            booster_params: int/str/float - Parameters for the booster.\n",
    "                See https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    "            early_stopping_rounds: int - Early stopping rounds (optional).\n",
    "        model (Output[Model]): Output model as a kfp Model object.\n",
    "            Attribute .path is the GCS location for the trained model\n",
    "            in XGBoost bst format\n",
    "        metrics_artifact (Output[Artifact]): Output metrics of all iterations for\n",
    "            the trained model in JSON format.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import os\n",
    "    import logging\n",
    "    import pandas\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    # numeric features in data for reference\n",
    "    NUM_COLS = [\"dayofweek\", \"hourofday\", \"trip_distance\", \"trip_miles\", \"trip_seconds\"]\n",
    "\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "    logging.info(\"Read train & validation data into pandas dataframes\")\n",
    "    train_df = pandas.read_csv(training_data)\n",
    "    valid_df = pandas.read_csv(validation_data)\n",
    "\n",
    "    logging.info(\"Split train/validation data into features & labels\")\n",
    "    X_train, y_train = (\n",
    "        train_df.drop(columns=[label_name]),\n",
    "        train_df[label_name],\n",
    "    )\n",
    "    X_valid, y_valid = (\n",
    "        valid_df.drop(columns=[label_name]),\n",
    "        valid_df[label_name],\n",
    "    )\n",
    "\n",
    "    logging.info(\"Build XGBoost model\")\n",
    "    xgb_model = XGBRegressor(**model_params)\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train, eval_set=[(X_valid, y_valid)]\n",
    "    )\n",
    "\n",
    "    logging.info(\"Save evaluation results to a dictionary\")\n",
    "    evals_result = xgb_model.evals_result()\n",
    "\n",
    "    # ensure to change GCS to local mount path\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "\n",
    "    logging.info(f\"Save model to: {model.path}\")\n",
    "    xgb_model.save_model(model.path + \"/model.bst\")\n",
    "\n",
    "    logging.info(f\"Save metrics to: {metrics_artifact.path}\")\n",
    "    with open(metrics_artifact.path, \"w\") as fp:\n",
    "        json.dump(evals_result, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb98beeb-4597-4655-94ec-7f1b790fb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a custom component to add the model to an UnManagedContainerModel artifact, so that it can be uploaded as a Google.VertexModel later. See here for reference:\n",
    "# https://cloud.google.com/vertex-ai/docs/pipelines/use-components#use_python_function-based_components\n",
    "@component(base_image='python:3.7',packages_to_install=['google-cloud-aiplatform'])\n",
    "def return_unmanaged_model(\n",
    "    trained_model: Input[Model],\n",
    "    model: Output[Artifact]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Fetch a trained model to be uploaded and attach serving image.\n",
    "    Args:\n",
    "        trained_model (Input[Model]): Model to be uploaded.\n",
    "        model (Output[Artifact]): Artifact that can be uploaded as a Google.VertexModel object.\n",
    "            Currently KFP doesn't support outputting artifacts in Google namespace,\n",
    "            so we need to manipulate the base type Artifact instead.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    model.metadata['containerSpec'] = {\n",
    "      'imageUri':\n",
    "          \"europe-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-4:latest\"\n",
    "          }\n",
    "    model.uri = trained_model.uri.rsplit(\"/\", 1)[0] + \"/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec0903-77fd-4fc4-a7a0-34632ae50caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"custom-model-evaluation\", pipeline_root=pipeline_root)\n",
    "def pipeline(\n",
    "):    \n",
    "    # train xgboost model\n",
    "    model_params = dict(\n",
    "        n_estimators=200,\n",
    "        early_stopping_rounds=10,\n",
    "        objective=\"reg:squarederror\",\n",
    "        booster=\"gbtree\",\n",
    "        learning_rate=0.3,\n",
    "        min_split_loss=0,\n",
    "        max_depth=6,\n",
    "    )\n",
    "\n",
    "    train_model = (\n",
    "        custom_train_job(\n",
    "            training_data=gcs_training_data,\n",
    "            validation_data=gcs_validation_data,\n",
    "            label_name=label_name,\n",
    "            model_params=json.dumps(model_params),\n",
    "            # Training wrapper specific parameters\n",
    "            project=project_id,\n",
    "            location=location,\n",
    "        )\n",
    "        .set_display_name(\"Vertex Training for XGB model\")\n",
    "    )\n",
    "\n",
    "    xgboost_model = train_model.outputs[\"model\"]\n",
    "   \n",
    "    unmanaged_model_op = return_unmanaged_model(trained_model=xgboost_model)\n",
    "    \n",
    "    model_upload_task = ModelUploadOp(\n",
    "        project = project_id,\n",
    "        location = location,\n",
    "        display_name = \"xgboost_chicago_regressor\",\n",
    "        unmanaged_container_model=unmanaged_model_op.outputs['model'])\n",
    "    \n",
    "    # Run Batch Explanations\n",
    "    batch_explain_task = ModelBatchPredictOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "        job_display_name=\"xgboost-regressor-batch-predict-evaluation\",\n",
    "        bigquery_source_input_uri=bq_test_data_no_label,\n",
    "        instances_format=\"bigquery\",\n",
    "        predictions_format=\"bigquery\",\n",
    "        bigquery_destination_output_uri=f\"bq://{project_id}.{dataset_id}\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        # Set the explanation parameters\n",
    "        generate_explanation=True,\n",
    "    )\n",
    "    \n",
    "    # Run evaluation based on prediction type and feature attribution component.\n",
    "    # After, import the model evaluations to the Vertex AI Model resource.\n",
    "    eval_task = ModelEvaluationRegressionOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        root_dir=pipeline_root,\n",
    "        predictions_bigquery_source=batch_explain_task.outputs[\"bigquery_output_table\"],\n",
    "        ground_truth_format=\"bigquery\",\n",
    "        ground_truth_bigquery_source=bq_test_data,\n",
    "        predictions_format=\"bigquery\",\n",
    "        prediction_score_column=\"prediction\",\n",
    "        target_field_name=label_name,\n",
    "        dataflow_service_account=service_account,\n",
    "        dataflow_subnetwork=subnetwork,\n",
    "    )\n",
    "    \n",
    "    # Get Feature Attributions\n",
    "    feature_attribution_task = ModelEvaluationFeatureAttributionOp(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        root_dir=pipeline_root,\n",
    "        predictions_format=\"bigquery\",\n",
    "        predictions_bigquery_source=batch_explain_task.outputs[\"bigquery_output_table\"],\n",
    "        dataflow_service_account=service_account,\n",
    "        dataflow_subnetwork=subnetwork,\n",
    "    )\n",
    "\n",
    "    # Import evaluations into UI\n",
    "    ModelImportEvaluationOp(\n",
    "        display_name=\"XGBoost Metrics\",\n",
    "        regression_metrics=eval_task.outputs[\"evaluation_metrics\"],\n",
    "        feature_attributions=feature_attribution_task.outputs[\"feature_attributions\"],\n",
    "        model=model_upload_task.outputs[\"model\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a9342-0de0-499c-9cbc-8b562ebf137d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "custom_train_job = create_custom_training_job_op_from_component(\n",
    "        component_spec=train_xgboost_model,\n",
    "        replica_count=1,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"custom_model_evaluation.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465ba434-93d8-4893-b804-b6064f317cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the pipeline\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"custom-model-evaluation\",\n",
    "    template_path=\"custom_model_evaluation.json\",\n",
    "    location=location,\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values={\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae2338-37c9-46ff-b9aa-df4166cc1429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the job\n",
    "job.submit(\n",
    "    service_account=service_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d8c7b2-70cd-489f-b9a1-fa4fd5afd679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
